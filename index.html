<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Octopi: Object Property Reasoning with Large
Tactile-Vision-Language Models">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Octopi</title>
<!-- 
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script> -->

<!--   <script>
     -->

   

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img src="media/emoji (5).png" alt="Emoji" style="width: 40px; height: 40px; vertical-align: middle;">Octopi: <br> Object Property Reasoning with Large Tactile-Vision-Language Models
          </h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://2023.emnlp.org/">ArXiv</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a target="_blank" href="https://samsonyubaijian.github.io/">Samson Yu</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://www.researchgate.net/profile/Lin-Kelvin">Kelvin Lin</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://anxingxiao.com/index.html">Anxing Xiao</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://duanjiafei.com/">Jiafei Duan</a><sup>2</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://haroldsoh.com/">Harold Soh</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National University of Singapore,</span>
            <span class="author-block"><sup>2</sup>University of Washington</span>
          </div>

          <span class="link-block"><a target="_blank" href="https://arxiv.org/abs/2310.07018" class="external-link button is-normal is-rounded is-dark"><span class="icon"><i class="fas fa-file"></i></span><span>ArXiv</span></a></span>

          <!-- Code Link. -->
          <span class="link-block"><a target="_blank" href="https://github.com/NewtonReasoning/Newton" class="external-link button is-normal is-rounded is-dark"><span class="icon"><i class="fab fa-github"></i></span><span>Code</span></a></span>
          <span class="link-block">
          <a target="_blank" href="https://huggingface.co/datasets/NEWTONReasoning/NEWTON/viewer/default/confident_questions?fbclid=IwAR23871OjV50d2tPBGnAKgmFljoAR78GwSIlcSPXd60h0jFjdKhBGb_IlIA" class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-database" aria-hidden="true"></i>
            </span>
            <span>Dataset</span>
          </a>
        </span>
          <br><br>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered is-centered">
          <br>
          <img src="media/Untitled1080p79-ezgif.com-video-to-gif-converter.gif" class="interpolation-image" alt="Interpolate start reference image." width="960" height="540" />

        </div>
        <h2 class="subtitle has-text-centered">
          <span class="dperact">Octopi</span> is an end-to-end system that leverages both tactile representation learning and large vision-language models to predict and reason about vision-based tactile inputs with minimal language fine-tuning.
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop height="100%">
            <source src="media/intro/v2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay muted loop height="100%">
            <source src="media/intro/v9.mp4"
                    type="video/mp4">
          </video>
        </div>
         <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/v6.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop height="100%">
            <source src="media/intro/a3.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/a2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay muted loop height="100%">
            <source src="media/intro/v5.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/a1.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay muted loop height="100%">
            <source src="media/intro/a4.mp4"
                    type="video/mp4">
          </video>
        </div>
         <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay muted loop height="100%">
            <source src="media/intro/a5.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<h2 class="subtitle has-text-centered">
</br>
  We use <b>AR2-D2</b> to collect robot demonstrations cross three manipulation tasks (<em>press</em>, <em>pick-up</em> and <em>push</em>) on <b>9</b> personalized objects. </br>These personalized objects are uniquely <b>shaped, sized, or textured items designed to meet the specific needs or functionalities</b> of individual users within their personalized environments.
</h2>
<br>
 -->


<!-- <section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="100%">
            <source src="media/intro/ARDemo.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        <span class="dperact">AR2-D2's</span> demonstrations collection interface via an iPhone/iPad. 
        </h2>
      </div>
    </div>
  </div>
</section> -->
                             




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
<!--           </br>
          <img src="media/9.png" class="interpolation-image" 
           alt="Interpolate start reference image." />
          </br> -->
          <br/>
          <p>
          </p>
          Physical reasoning is essential for safe and efficient AI systems in robotic manipulation. To ensure that the systemâ€™s physical understanding of the object properties is grounded in its existing physical reality, computer vision is insufficient. Recent work in physical reasoning focuses on vision-language inputs without the use of tactile inputs, with language serving as a powerful tool for abstraction and communication of additional contexts. While these works have demonstrated success in diverse forms of physical reasoning, they focus on physical properties that can be effectively deciphered from visual inputs and language without any interaction. In addition, systems that are built specifically for robotic manipulation either perform ineffectively in visually ambiguous scenarios with only vision-language inputs or focus on simplified scenarios. In this work, we first define a set of physical properties that are both useful and decipherable from our choice of tactile sensor, GelSight, and introduce Feelang, a tactile video dataset annotated using the physical properties. We then introduce Octopi, an end-to-end system that leverages both tactile representation learning and large vision-language models to predict and reason about vision-based tactile inputs with minimal language fine-tuning. We construct an evaluation suite spanning five problems to evaluate our system's learned tactile representations and physical reasoning holistically. Our experiments show that Octopi is able to effectively use intermediate physical property prediction to improve physical reasoning robustness and improve performance on robotic tasks when there is visual ambiguity.
        </div>
      </div>
    </div>
    <br>
   
    <!--/ Abstract. -->

  </div>


</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">

<style>
  .centered-container {
    display: flex;
    flex-direction: column;
    align-items: center;
    text-align: center;
  }

  .interpolation-image {
    display: block;
    margin: 20px 0;  /* Give some space above and below the image */
  }
</style>

<section>
  <div class="rows is-centered">
    <div class="row is-full-width centered-container">
      <h2 class="title is-3"><span class="dperact">Octopi</span></h2>
      <h3 class="title is-4">The first tactile VLVM</h3>
      <p>
        To facilitate the grounding of our physical reasoning on tactile inputs, we collected a tactile dataset of 74 everyday objects, totalling 408 tactile videos. These objects were selected to span across our three selected properties, with variations across object types and materials.
      </p>
      <img src="media/pipeline (1).png" class="interpolation-image" alt="Interpolate start reference image." />
      <h3 class="title is-4">Freelang dataset</h3>
      <img src="media/dataset.png" class="interpolation-image" alt="Interpolate start reference image." />
      <h3 class="title is-4">Model architecture</h3>
      <img src="media/model.png" class="interpolation-image" alt="Interpolate start reference image." />
      <p> The Octopi framework comprises three trained components: 1) tactile input encoder, 2) projection module, and 3) LLM, following prior LVLM work that take videos as inputs. Firstly, prior LVLM work leverage the strengths of powerful pre-trained vision models, specifically CLIP's visual encoder, to extract useful feature representations. We follow prior work and use the pre-trained CLIP visual encoder ViT-L/14 as the base model of our tactile encoder. A projection module is present in existing work to project the encoder's output representations into embeddings with the same dimensionality as the LLM's word embeddings. This projection module is generally simple and comprises one or two trainable layers. For our projection module, we follow LLaVA's projection module and use two linear layers along with an intermediate GELU activation. Finally, the LLM forms the language understanding component of Octopi. LLM performance depends heavily on the datasets that they are pre-trained on. We use the open-source LLaMA-based LLM, Vicuna due to its dialogue capabilities. </p>
      <h2 class="title is-3">Results</h2>
      <p>Results for <b>Track 1</b></p>
      <img src="media/result_all.png" class="interpolation-image" alt="Interpolate start reference image." />
   
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3 centered-text">Ablation Studies</h2>
      <p class="centered-text">We provide an analysis of the NEWTON dataset, focusing on potential ways of leveraging \dataset to enhance model performance in a physical reasoning context, and examining the consistency of LLMs with regard to model size, question polarity, and answer positioning. </p>
          <img src="media/8.png" class="interpolation-image" 
           alt="Interpolate start reference image." />
          </br> 
            
          
        <br/>

      

      

    </div>

  </div>

</section>

<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2023newton,
  title     = {NEWTON: Are Large Language Models Capable of Physical Reasoning?}, 
  author    = {Wang, Yi Ru and Duan, Jiafei and Fox, Dieter and Srinivasa, Siddhartha,
  booktitle = {arXiv preprint arXiv:2310.07018},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            This website template borrowed from NeRFies made by the amazing Keunhong Park.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
