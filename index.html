<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Octopi: Object Property Reasoning with Large Tactile-Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Octopi: Object Property Reasoning with Large Tactile-Language Models</title>   

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">


<style>
  .centered-container {
    display: flex;
    flex-direction: column;
    align-items: center;
    text-align: center;
  }

  .interpolation-image {
    display: block;
    margin: 20px 0;  /* Give some space above and below the image */
  }
</style>


<!-- Intro -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img src="media/emoji.png" alt="Emoji" style="width: 40px; height: 40px; vertical-align: middle;">Octopi
          </h1>
          <h3 class="title is-3 publication-subtitle">Object Property Reasoning with Large Tactile-Language Models</h3>
          <h4 class="title is-4 conference"><a target="_blank" href="https://roboticsconference.org/">RSS 2024</a></h4>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a target="_blank" href="https://samsonyubaijian.github.io/">Samson Yu</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://www.researchgate.net/profile/Lin-Kelvin">Kelvin Lin</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://anxingxiao.com/index.html">Anxing Xiao</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://duanjiafei.com/">Jiafei Duan</a><sup>2</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://haroldsoh.com/">Harold Soh</a><sup>1</sup></span>
          </div>
          <!-- Organizations -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National University of Singapore,</span>
            <span class="author-block"><sup>2</sup>University of Washington</span>
          </div>
          <!-- Links -->
          <span class="link-block"><a target="_blank" href="https://arxiv.org/abs/2405.02794" class="external-link button is-normal is-rounded is-dark"><span class="icon"><i class="fas fa-file"></i></span><span>Paper</span></a></span>
          <span class="link-block"><a target="_blank" href="https://github.com/clear-nus/octopi" class="external-link button is-normal is-rounded is-dark"><span class="icon"><i class="fab fa-github"></i></span><span>Code + Dataset</span></a></span>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Summary and video -->
<section class="hero teaser">
  <div class="container is-fullhd">
        <h2 class="subtitle has-text-centered">
          <b>Summary</b>: Large VLM physical property prediction and scenario reasoning with GelSight tactile videos.
        </h2>
        <br>
        <div class="columns is-vcentered is-centered">
          <br>
          <img src="media/robot.gif" class="robot-demo" alt="Robot demonstration." width="640" height="360" />

        </div>
    <br>
  </div>
</section>


<!-- Overview -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <h3 class="title is-3 overview">Overview</h3>
          <div class="content has-text-justified">
            <p>
            We introduce <b>PhysiCLeAR</b>, a new dataset containing 408 GelSight tactile videos of 74 everyday household objects. The videos are collected by hand
            with two exploratory procedures, pressing and rotation, and annotated for three useful physical properties: <em>hardness</em>,
            <em>roughness</em> and <em>bumpiness</em>. PhysiCLeAR leverages the videos and annotations to create five language-driven physical description and understanding
            tasks. We train and evaluate <b>Octopi</b>, a large VLM, on PhysiCLeAR for tactile-grounded physical understanding and scenario reasoning. Our experiments show that Octopi is able to predict physical properties from the tactile videos accurately and use the physical properties to reason about and resolve scenarios.
            </p>
            <br>
          </div>
          <img src="media/pipeline.png" class="interpolation-image" alt="Model pipeline." />
        </div>
      </div>
    </div>
    <br>
  </div>
</section>


<!-- PhysiCLeAR -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div>
        <br>
        <h3 class="title is-3">PhysiCLeAR: Tactile Dataset</h3>
        <img src="media/items.png" class="interpolation-image" alt="Dataset items." />
        <br>
        <div class="content has-text-justified">
          <p>
            A new GelSight tactile dataset with property diversity, object diversity, and material diversity for three useful physical properties: hardness, roughness and bumpiness.
            of 74 everyday objects, totalling 408 tactile videos. These objects were selected to span across our three selected properties, with variations across object types and materials.
          </p>
        </div>
        <br>
        <img src="media/dataset.png" class="interpolation-image" alt="Dataset features." />
      </div>
    </div>
    <br>
    <div class="columns is-centered has-text-centered">
      <h4 class="title is-4">Example Tactile Images</h4>
    </div>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-steve">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/basket.jpg" id="steve" style="height:100%;" alt="Hand Sanitization">
      </div>
      <div class="item item-fullbody">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/clothes_peg.jpg" id="fullbody" style="height:100%;" alt="Food Bin">
      </div>
      <div class="item item-shiba">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/egg.jpg" id="shiba" style="height:100%;" alt="Drawer">
      </div>
      <div class="item item-blueshirt">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/gauze_pad.jpg" id="blueshirt" style="height:100%;" alt="Marker">
      </div>
      <!-- Assuming each item has a unique image now, removed additional "shiba" items for brevity -->
      <div class="item item-chair-tp">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/hairbrush_bristles_side.jpg" id="chair-tp" style="height:100%;" alt="Blocks">
      </div>
      <div class="item item-blueshirt">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/insulating_holder.jpg" id="blueshirt" style="height:100%;" alt="Marker">
      </div>
      <div class="item item-blueshirt">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/potato.jpg" id="blueshirt" style="height:100%;" alt="Marker">
      </div>
      <div class="item item-blueshirt">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/unripe_avocado.jpg" id="blueshirt" style="height:100%;" alt="Marker">
      </div>
      <div class="item item-blueshirt">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/tsa_lock_numbers.jpg" id="blueshirt" style="height:100%;" alt="Marker">
      </div>
    </div>
    <br>
  </div>
</section>


<!-- Octopi -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="rows is-centered">
        <div class="row is-full-width centered-container">
          <br>
          <h3 class="title is-3">Octopi: Tactile VLM</h3>
          <img src="media/model.png" class="interpolation-image" alt="Interpolate start reference image." />
          <p> The Octopi framework comprises three trained components: 1) tactile input encoder, 2) projection module, and 3) LLM, following prior LVLM work that take videos as inputs. Firstly, prior LVLM work leverage the strengths of powerful pre-trained vision models, specifically CLIP's visual encoder, to extract useful feature representations. We follow prior work and use the pre-trained CLIP visual encoder ViT-L/14 as the base model of our tactile encoder. A projection module is present in existing work to project the encoder's output representations into embeddings with the same dimensionality as the LLM's word embeddings. This projection module is generally simple and comprises one or two trainable layers. For our projection module, we follow LLaVA's projection module and use two linear layers along with an intermediate GELU activation. Finally, the LLM forms the language understanding component of Octopi. LLM performance depends heavily on the datasets that they are pre-trained on. We use the open-source LLaMA-based LLM, Vicuna due to its dialogue capabilities. </p>
        </div>
      </div>
    </div>
    <br>
  </div>
</section>


<!-- Results -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <h2 class="title is-3">Results</h2>
      <img src="media/results.png" class="interpolation-image" style="width:60%; height:auto;" alt="Interpolate start reference image." />
    </div>
  </div>
</section>


<!-- Scenario reasoning -->
<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <h2 class="title is-3 centered-text">Scenario Reasoning Examples</h2>
      <p class="centered-text">We showcase two different dialogue scenarios that Octopi is capable of. First being food state reasoning, and second being objet part reasoning. </p>
      <img src="media/scenario_reasoning/rice.png" class="interpolation-image" style="width:80%; height:auto;" alt="Interpolate start reference image." />
      <br>
      <img src="media/scenario_reasoning/part.png" class="interpolation-image" style="width:80%; height:auto;" alt="Interpolate start reference image." />
    </div>
    <br>
  </div>
</section>


<!-- Citation
<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2023newton,
  title     = {NEWTON: Are Large Language Models Capable of Physical Reasoning?}, 
  author    = {Wang, Yi Ru and Duan, Jiafei and Fox, Dieter and Srinivasa, Siddhartha,
  booktitle = {arXiv preprint arXiv:2310.07018},
  year      = {2023},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            This website template is borrowed from Nerfies made by the amazing Keunhong Park.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
