<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Octopi: Object Property Reasoning with Large Tactile-Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Octopi: Object Property Reasoning with Large Tactile-Language Models</title>   

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">


<style>
  .centered-container {
    display: flex;
    flex-direction: column;
    align-items: center;
    text-align: center;
  }

  .interpolation-image {
    display: block;
    margin: 20px 0;  /* Give some space above and below the image */
  }
</style>


<!-- Intro -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img src="media/emoji.png" alt="Emoji" style="width: 40px; height: 40px; vertical-align: middle;">Octopi
          </h1>
          <h3 class="title is-3 publication-subtitle">Object Property Reasoning with Large Tactile-Language Models</h3>
          <h4 class="title is-4 conference">RSS 2024</h4>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a target="_blank" href="https://samsonyubaijian.github.io/">Samson Yu</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://www.researchgate.net/profile/Lin-Kelvin">Kelvin Lin</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://anxingxiao.com/index.html">Anxing Xiao</a><sup>1</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://duanjiafei.com/">Jiafei Duan</a><sup>2</sup>,</span>
            <span class="author-block"><a target="_blank" href="https://haroldsoh.com/">Harold Soh</a><sup>1</sup></span>
          </div>
          <!-- Organizations -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National University of Singapore,</span>
            <span class="author-block"><sup>2</sup>University of Washington</span>
          </div>
          <!-- Links -->
          <span class="link-block"><a target="_blank" href="https://arxiv.org/abs/2405.02794" class="external-link button is-normal is-rounded is-dark"><span class="icon"><i class="fas fa-file"></i></span><span>Paper</span></a></span>
          <span class="link-block"><a target="_blank" href="https://github.com/clear-nus/octopi" class="external-link button is-normal is-rounded is-dark"><span class="icon"><i class="fab fa-github"></i></span><span>Code + Dataset</span></a></span>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Summary and video -->
<section class="hero teaser">
  <div class="container is-fullhd">
        <h2 class="subtitle has-text-centered">
          <b>Summary</b>: Large VLM physical property prediction and scenario reasoning with GelSight tactile videos.
        </h2>
        <br>
        <div class="columns is-vcentered is-centered">
          <br>
          <img src="media/robot.gif" class="robot-demo" alt="Robot demonstration." width="640" height="360" />
        </div>
    <br>
  </div>
</section>


<!-- Overview -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <h3 class="title is-3 overview">Overview</h3>
          <div class="content has-text-justified">
            <p>
            We introduce <b>PhysiCLeAR</b>, a new dataset containing GelSight tactile videos of everyday household objects. The videos are collected by hand
            with two exploratory procedures, pressing and rotation, and annotated for three useful physical properties: hardness, roughness and bumpiness. PhysiCLeAR leverages the videos and annotations to create five language-driven physical description and understanding tasks. We train and evaluate <b>Octopi</b>, a large VLM, on PhysiCLeAR for tactile-grounded physical understanding and scenario reasoning.
            </p>
          </div>
          <img src="media/pipeline.png" class="interpolation-image" alt="Model pipeline." />
          <div class="content has-text-justified">
            <br>
            <p>
            Our experiments show that Octopi is able to predict physical properties from the tactile videos accurately and use the physical properties to reason about and resolve scenarios.
            </p>
          </div>
        </div>
      </div>
    </div>
    <br>
  </div>
</section>


<!-- PhysiCLeAR -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div>
        <br>
        <h3 class="title is-3">PhysiCLeAR: Tactile Dataset</h3>
        <div class="content has-text-justified">
          <p>
            A new GelSight tactile dataset with property diversity, object diversity, and material diversity for three useful physical properties: hardness, roughness and bumpiness. It contains <b>74</b> everyday household objects and <b>408</b> tactile videos. The videos are annotated by three annotators for the physical properties.
          </p>
        </div>
        <div class="columns is-vcentered is-centered">
          <img src="media/items.png" class="interpolation-image" alt="Dataset items." width="640" height="360" />
        </div>
      </div>
    </div>
    <br>
    <div class="columns is-centered has-text-centered">
      <h4 class="title is-4">Example Tactile Images</h4>
    </div>
    <div id="results-carousel" class="carousel results-carousel">
      <div class="item item-basket">
        <!-- Replace video with an image -->
        <div class="card-image" style="height:80%;">
          <img src="media/tactile_images/basket.jpg" id="basket" style="height:80%;" alt="Basket.">
        </div>
        <div class="content is-centered has-text-centered">Basket</div>
      </div>
      <div class="item item-clothes-peg">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/clothes_peg.jpg" id="clothes-peg" style="height:80%;" alt="Clothes peg.">
      </div>
      <div class="item item-egg">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/egg.jpg" id="egg" style="height:80%;" alt="Egg.">
      </div>
      <div class="item item-gauze-pad">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/gauze_pad.jpg" id="gauze-pad" style="height:80%;" alt="Gauze pad.">
      </div>
      <div class="item item-hairbrush-bristles-side">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/hairbrush_bristles_side.jpg" id="hairbrush-bristles-side" style="height:80%;" alt="Hairbrush bristles side.">
      </div>
      <div class="item item-insulating-holder">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/insulating_holder.jpg" id="insulating-holder" style="height:80%;" alt="Insulating holder.">
      </div>
      <div class="item item-potato">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/potato.jpg" id="potato" style="height:80%;" alt="Potato.">
      </div>
      <div class="item item-unripe-avocado">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/unripe_avocado.jpg" id="unripe-avocado" style="height:80%;" alt="Unripe avocado.">
      </div>
      <div class="item item-tsa-lock-numbers">
        <!-- Replace video with an image -->
        <img src="media/tactile_images/tsa_lock_numbers.jpg" id="tsa-lock-numbers" style="height:80%;" alt="TSA lock numbers.">
      </div>
    </div>
    <br><br>
    <div class="columns is-centered has-text-centered">
      <h4 class="title is-4">Physical Property Details</h4>
    </div>
    <div class="content has-text-justified">
      <p>
        The physical object properties selected, along with their descriptions and semantic categories.
      </p>
    </div>
    <div>
      <div class="columns is-vcentered is-centered">
        <img src="media/physical_properties.png" class="interpolation-image" alt="Dataset features." width="800" height="450" />
      </div>
    </div>
    <br><br>
    <div class="columns is-centered has-text-centered">
      <h4 class="title is-4">GelSight Dataset Comparisons</h4>
    </div>
    <div class="content has-text-justified">
      <p>
        PhysiCLeAR provides physical property labels for tactile descriptions and physical reasoning across three physical properties. We further compare against existing datasets across three diversity measures. Property diversity refers to whether there are objects in the dataset that vary across the three properties we selected: hardness, roughness and bumpiness. Object diversity indicates whether there is more than one type of object in the dataset. Material diversity indicates the number of different materials in the dataset.
      </p>
    </div>
    <div>
      <div class="columns is-vcentered is-centered">
        <img src="media/gelsight_dataset_comparisons.png" class="interpolation-image" alt="Dataset features." width="800" height="450" />
      </div>
    </div>
    <br><br>
    <div class="columns is-centered has-text-centered">
      <h4 class="title is-4">LLM Training & Evaluation Suite</h4>
    </div>
    <div class="content has-text-justified">
      <p>
        PhysiCLeAR also contains five physical description and understanding tasks. We give each task's motivation and indicate whether they are used for Octopi's training and/or evaluation. Specific details about the prompt setup of each task can be found in our paper.
      </p>
    </div>
    <div>
      <div class="columns is-vcentered is-centered">
        <img src="media/llm_tasks.png" class="interpolation-image" alt="LLM training and evaluation suite." width="800" height="450" />
      </div>
    </div>
    <br>
  </div>
</section>


<!-- Octopi -->
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="rows is-centered">
        <div class="row is-full-width centered-container">
          <br>
          <h3 class="title is-3">Octopi: Tactile VLM</h3>
          <div class="content has-text-justified">
            <p> Octopi comprises three trained components: 1) tactile input encoder, 2) projection module, and 3) LLM, similar to prior LVLM models. We use the CLIP (ViT-L/14) visual encoder to extract feature representations from input tactile videos. The encoder's output is then mapped to the LLM's word embedding space using a projection module (two linear layers). Finally, the LLM forms the language understanding component of Octopi. We use the open-source LLaMA-based LLM, Vicuna v1.5, recognized for its dialogue capabilities. Language embeddings are derived through tokenization and then Vicuna's word embedding layer, with <code>&lt;tact_start&gt;</code> and <code>&lt;tact_end&gt;</code> being newly trained word embeddings indicating the start and end of a tactile frame sequence from a single tactile sensor.</p>
          </div>
          <div class="columns is-vcentered is-centered">
            <img src="media/model.png" class="interpolation-image" alt="Model diagram." width="800" height="450" />
          </div>
        </div>
        <div class="row is-full-width centered-container">
          <br>
          <h4 class="title is-4">Training Methodology</h4>
          <div class="content has-text-justified">
            <!-- <p> Octopi comprises three trained components: 1) tactile input encoder, 2) projection module, and 3) LLM, similar to prior LVLM models. We use the CLIP (ViT-L/14) visual encoder to extract feature representations from input tactile videos. The encoder's output is then mapped to the LLM's word embedding space using a projection module (two linear layers). Finally, the LLM forms the language understanding component of Octopi. We use the open-source LLaMA-based LLM, Vicuna v1.5, recognized for its dialogue capabilities. Language embeddings are derived through tokenization and then Vicuna's word embedding layer, with <code>&lt;tact_start&gt;</code> and <code>&lt;tact_end&gt;</code> being newly trained word embeddings indicating the start and end of a tactile frame sequence from a single tactile sensor.</p> -->
          </div>
          <div class="columns is-vcentered is-centered">
            <img src="media/training.png" class="interpolation-image" alt="Model diagram." width="800" height="450" />
          </div>
        </div>
      </div>
    </div>
    <br>
  </div>
</section>


<!-- Results and scenario reasoning -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="rows">
        <br>
        <h3 class="title is-3">Results</h3>
        <div class="columns is-centered has-text-centered">
          <h4 class="title is-4">Physical Property Prediction</h4>
        </div>
        <div class="content has-text-justified">
          <p>
            Octopi-7b and Octopi-13b perform above the random baseline for object property predictions and have similar performance to Fine-tuned CLIP + Classification, indicating that Octopi can be used for object property prediction. Octopi-13b has a higher combined accuracy (i.e. all three physical properties are correctly predicted for a given object) when compared to Octopi-7b, suggesting there are performance gains with larger LLMs for tactile signal grounding.
          </p>
        </div>
        <div>
          <div class="columns is-vcentered is-centered">
            <img src="media/property_prediction_results.png" class="interpolation-image" alt="Overall results." width="800" height="450" />
          </div>
        </div>
        <br><br>
        <div class="columns is-centered has-text-centered">
          <h4 class="title is-4">Scenario Reasoning</h4>
        </div>
        <div class="content has-text-justified">
          <p>
            
          </p>
        </div>
        <div>
          <div class="columns is-vcentered is-centered">
            <img src="media/scenario_results.png" class="interpolation-image" alt="Overall results." width="800" height="450" />
          </div>
        </div>
        <br><br>
        <h4 class="title is-4 centered-text">Scenario Reasoning Examples</h4>
        <div class="content has-text-justified">
          <p>
            
          </p>
        </div>
        <div class="columns is-vcentered is-centered">
          <div style="width:50%; float: left;">
            <img src="media/scenario_reasoning/food_state_reasoning.png" class="interpolation-image" style="width:100%; height:auto;" alt="Rice reasoning interaction." />
          </div>
          <div style="width:50%; float: right;">
            <img src="media/scenario_reasoning/object_part_reasoning.png" class="interpolation-image" style="width:100%; height:auto;" alt="Toothbrush part reasoning interaction." />
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h3 class="title">Citation</h3>
    If you use this work or find it helpful, please consider citing our work. <br><br>
    <pre><code>
      @article{yu2024octopi,
        title={Octopi: Object Property Reasoning with Large Tactile-Language Models},
        author={Yu, Samson and Lin, Kelvin and Xiao, Anxing and Duan, Jiafei and Soh, Harold},
        journal={arXiv preprint arXiv:2405.02794},
        year={2024}
      }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            This website template is borrowed from Nerfies made by the amazing Keunhong Park.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
